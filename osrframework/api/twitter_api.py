#!/usr/bin/env python
# encoding: utf-8
#
##################################################################################
#
#    Copyright 2015 FÃ©lix Brezo and Yaiza Rubio (i3visio, contacto@i3visio.com)
#
#    This file is part of OSRFramework. You can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##################################################################################

import argparse
import json
import time
import tweepy #https://github.com/tweepy/tweepy
import csv

import osrframework.utils.config_api_keys as api_keys
from osrframework.utils.global_api import APIWrapper as APIWrapper

class TwitterAPIWrapper(APIWrapper):
    """
        Twitter API wrapper using tweepy API.
    """

    def __init__(self, api_data=None):
        """
            :param api_data:    dictionary containing the credentials for the given platform.
        """
        if api_data == None:
            api_data = api_keys.returnListOfAPIKeys()["twitter"]
        # Processing the results received by parameter
        self.consumer_key= api_data["consumer_key"]
        self.consumer_secret= api_data["consumer_secret"]
        self.access_key= api_data["access_key"]
        self.access_secret = api_data["access_secret"]

        # The platformName, a bit redundant
        self.platformName = "Twitter"

    def _connectToAPI(self):
        """
            :return: A tweepy.API object that performs the queries
        """
        #authorize twitter, initialize tweepy
        auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)
        auth.set_access_token(self.access_key, self.access_secret)
        api = tweepy.API(auth)
        return api

    def _rate_limit_status(self, api=None, mode=None):
        """
            Verifying the API limits
        """
        if api == None:
            api = self.connectToAPI()

        if mode == None:
            print json.dumps(api.rate_limit_status(), indent=2)
            raw_input("<Press ENTER>")
        else:
            # Testing if we have enough queries
            while True:
                allLimits = api.rate_limit_status()
                if mode == "get_user":
                    limit = allLimits["resources"]["users"]["/users/show/:id"]["limit"]
                    remaining = allLimits["resources"]["users"]["/users/show/:id"]["remaining"]
                    reset = allLimits["resources"]["users"]["/users/show/:id"]["reset"]
                elif mode == "get_followers":
                    limit = allLimits["resources"]["followers"]["/followers/ids"]["limit"]
                    remaining = allLimits["resources"]["followers"]["/followers/ids"]["remaining"]
                    reset = allLimits["resources"]["followers"]["/followers/ids"]["reset"]
                elif mode == "get_friends":
                    limit = allLimits["resources"]["friends"]["/friends/ids"]["limit"]
                    remaining = allLimits["resources"]["friends"]["/friends/ids"]["remaining"]
                    reset = allLimits["resources"]["friends"]["/friends/ids"]["reset"]
                elif mode == "search_users":
                    limit = allLimits["resources"]["users"]["/users/search"]["limit"]
                    remaining = allLimits["resources"]["users"]["/users/search"]["remaining"]
                    reset = allLimits["resources"]["users"]["/users/search"]["reset"]
                else:
                    remaining = 1
                """elif mode == "get_all_docs":
                    limit = allLimits["resources"]REPLACEME["limit"]
                    remaining = allLimits["resources"]REPLACEME["remaining"]
                    reset = allLimits["resources"]REPLACEME["reset"]"""
                """elif mode == "get_users":
                    limit = allLimits["resources"]REPLACEME["limit"]
                    remaining = allLimits["resources"]REPLACEME["remaining"]
                    reset = allLimits["resources"]REPLACEME["reset"] """
                """else:
                    remaining = 1"""
                # Checking if we have enough remaining queries
                if remaining > 0:
                    #raw_input(str(remaining) + " queries yet...")
                    break
                else:
                    waitTime = 60
                    print "No more queries remaining, sleeping for " + str(waitTime) +" seconds..."
                    time.sleep(waitTime)

        return 0

    def _processUser(self, jUser):
        """
            Convert tweepy.User to a i3visio-like user. This will process the returned JSON object that the API returns to transform it to the i3visio-like format. A sample answer is copied now when testing it to the @i3visio user in Twitter.
{
  "follow_request_sent": false,
  "has_extended_profile": false,
  "profile_use_background_image": true,
  "profile_text_color": "333333",
  "default_profile_image": false,
  "id": 2594815981,
  "profile_background_image_url_https": "https://abs.twimg.com/images/themes/theme1/bg.png",
  "verified": false,
  "profile_location": null,
  "profile_image_url_https": "https://pbs.twimg.com/profile_images/491716630292881408/FBqYf9qv_normal.png",
  "profile_sidebar_fill_color": "DDEEF6",
  "entities": {
    "url": {
      "urls": [
        {
          "url": "http://t.co/Vus95W8ub6",
          "indices": [
            0,
            22
          ],
          "expanded_url": "http://www.i3visio.com",
          "display_url": "i3visio.com"
        }
      ]
    },
    "description": {
      "urls": [
        {
          "url": "http://t.co/SGty7or6SQ",
          "indices": [
            30,
            52
          ],
          "expanded_url": "http://github.com/i3visio/osrframework",
          "display_url": "github.com/i3visio/osrfra\u2026"
        }
      ]
    }
  },
  "followers_count": 21,
  "profile_sidebar_border_color": "C0DEED",
  "id_str": "2594815981",
  "profile_background_color": "C0DEED",
  "listed_count": 5,
  "status": {
    "lang": "es",
    "favorited": false,
    "entities": {
      "symbols": [],
      "user_mentions": [],
      "hashtags": [],
      "urls": []
    },
    "contributors": null,
    "truncated": false,
    "text": "Podemos confirmar que Alpify, aunque acabe en ...fy no es una aplicaci\u00f3n nuestra. ;) \u00a1A aprovechar lo que queda de domingo!",
    "created_at": "Sun Aug 16 17:35:37 +0000 2015",
    "retweeted": true,
    "in_reply_to_status_id_str": null,
    "coordinates": null,
    "in_reply_to_user_id_str": null,
    "source": "<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>",
    "in_reply_to_status_id": null,
    "in_reply_to_screen_name": null,
    "id_str": "632968969662689280",
    "place": null,
    "retweet_count": 1,
    "geo": null,
    "id": 632968969662689280,
    "favorite_count": 0,
    "in_reply_to_user_id": null
  },
  "is_translation_enabled": false,
  "utc_offset": null,
  "statuses_count": 56,
  "description": "Leading OSRFramework project (http://t.co/SGty7or6SQ) for researching in Open Sources. #security #osint #socialengineering",
  "friends_count": 10,
  "location": "Espa\u00f1a",
  "profile_link_color": "0084B4",
  "profile_image_url": "http://pbs.twimg.com/profile_images/491716630292881408/FBqYf9qv_normal.png",
  "following": true,
  "geo_enabled": false,
  "profile_background_image_url": "http://abs.twimg.com/images/themes/theme1/bg.png",
  "name": "i3visio",
  "lang": "en",
  "profile_background_tile": false,
  "favourites_count": 6,
  "screen_name": "i3visio",
  "notifications": false,
  "url": "http://t.co/Vus95W8ub6",
  "created_at": "Sun Jun 29 13:27:20 +0000 2014",
  "contributors_enabled": false,
  "time_zone": null,
  "protected": false,
  "default_profile": true,
  "is_translator": false
}

            :param jUser:   A Json representing the information of a profile as returned by the API.


            :return: Dict in i3visio-like format.
        """
        #raw_input(json.dumps(jUser, indent=2))
        r = {}
        r["type"] = "i3visio.profile"
        r["value"] = self.platformName + " - " + jUser["screen_name"]
        r["attributes"] = []

        # Appending platform URI
        """aux = {}
        aux["type"] = "i3visio.uri"
        aux["value"] = qURL
        aux["attributes"] = []
        r["attributes"].append(aux) """
        # Appending the id
        aux = {}
        aux["type"] = "@twitter_id"
        aux["value"] = jUser["id_str"]
        aux["attributes"] = []
        r["attributes"].append(aux)

        # Appending the alias
        aux = {}
        aux["type"] = "i3visio.alias"
        aux["value"] = jUser["screen_name"]
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending fullname
        aux = {}
        aux["type"] = "i3visio.fullname"
        aux["value"] = jUser["name"]
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending description
        aux = {}
        aux["type"] = "i3visio.text"
        aux["value"] = jUser["description"] if jUser["description"] != "" else "[N/A]"
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending platform name
        aux = {}
        aux["type"] = "i3visio.platform"
        aux["value"] = self.platformName
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending location
        aux = {}
        aux["type"] = "i3visio.location"
        aux["value"] = jUser["location"] if jUser["location"] != "" else "[N/A]"
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending profile_location
        aux = {}
        aux["type"] = "i3visio.location.current"
        aux["value"] = jUser["profile_location"] if jUser["profile_location"] != None else "[N/A]"
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending uri homepage
        try:
            urls = jUser["entities" ]["url"]["urls"]
            for url in urls:
                aux = {}
                aux["type"] = "i3visio.uri.homepage"
                aux["value"] = url["expanded_url"] if url["expanded_url"] != None else "[N/A]"
                aux["attributes"] = []
                r["attributes"].append(aux)
        except Exception as e:
            #Something happenned when parsing the URLS
            aux = {}
            aux["type"] = "i3visio.uri.homepage"
            aux["value"] = "[N/A]"
            aux["attributes"] = []
            r["attributes"].append(aux)
        # Appending profile uri homepage
        try:
            aux = {}
            aux["type"] = "i3visio.uri.image.profile"
            aux["value"] = jUser["profile_image_url"] if jUser["profile_image_url"] != None else "[N/A]"
            aux["attributes"] = []
            r["attributes"].append(aux)
        except Exception as e:
            #Something happenned when parsing the Profile URL
            aux = {}
            aux["type"] = "i3visio.uri.image.profile"
            aux["value"] = "[N/A]"
            aux["attributes"] = []
            r["attributes"].append(aux)
        # Appending uri background
        try:
            aux = {}
            aux["type"] = "i3visio.uri.image.background"
            aux["value"] = jUser["profile_background_image_url"] if jUser["profile_background_image_url"] != None else "[N/A]"
            aux["attributes"] = []
            r["attributes"].append(aux)
        except Exception as e:
            #Something happenned when parsing the background URL
            aux = {}
            aux["type"] = "i3visio.uri.image.background"
            aux["value"] = "[N/A]"
            aux["attributes"] = []
            r["attributes"].append(aux)
        # Appending created_at
        aux = {}
        aux["type"] = "@created_at"
        aux["value"] = jUser["created_at"]
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending friends_count
        aux = {}
        aux["type"] = "@friends_count"
        aux["value"] = str(jUser["friends_count"])
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending followers_count
        aux = {}
        aux["type"] = "@followers_count"
        aux["value"] = str(jUser["followers_count"])
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending protected
        aux = {}
        aux["type"] = "@protected"
        aux["value"] = str(jUser["protected"]).lower()
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending geo_enabled
        aux = {}
        aux["type"] = "@geo_enabled"
        aux["value"] = str(jUser["geo_enabled"]).lower()
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending language
        aux = {}
        aux["type"] = "@language"
        aux["value"] = jUser["lang"]
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending time_zone
        aux = {}
        aux["type"] = "@time_zone"
        aux["value"] = jUser["time_zone"] if jUser["time_zone"] != None else "[N/A]"
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending verified
        aux = {}
        aux["type"] = "@verified"
        aux["value"] = str(jUser["verified"]).lower()
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending listed_count
        aux = {}
        aux["type"] = "@listed_count"
        aux["value"] = str(jUser["listed_count"])
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending publications_count
        aux = {}
        aux["type"] = "@publications_count"
        aux["value"] = str(jUser["statuses_count"])
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending favourites_count
        aux = {}
        aux["type"] = "@favourites_count"
        aux["value"] = str(jUser["favourites_count"])
        aux["attributes"] = []
        r["attributes"].append(aux)
        # Appending suspended
        try:
            aux = {}
            aux["type"] = "@suspended"
            aux["value"] = str(jUser["suspended"]).lower()
            aux["attributes"] = []
            r["attributes"].append(aux)
        except:
            pass
        return r

    def get_all_docs(self, screen_name):
        """
            Method to get all the tweets emitted by a user.

            :param screen_name: The Twitter username.

            :return:    List of tweets.
        """
        def _getNewTweets(api, screen_name,count=200, oldest=None, waitTime=60):
            """
                MEthod that recovers the new tweets or waits until the number of remaining calls has been freed.

                :param api:     A valid and connected api.
                :param screen_name: screen_name of the user to monitor.
                :param count:   Number of tweets to grab per iteration.
                :param oldes:  Oldest tweet to grab in this iteration.
                :param waitTime:    Number of seconds to wait between tries.

                :return:  List of new_tweets
            """
            # Verifying the limits of the API
            #self._rate_limit_status(api=api, mode="get_all_docs")

            waiting = True
            while waiting == True:
                try:
                    if oldest != None:
                        # We have to update the oldest id
                        new_tweets = api.user_timeline(screen_name=screen_name, count=count, max_id=oldest)
                    else:
                        new_tweets = api.user_timeline(screen_name=screen_name, count=count)
                    waiting = False
                    #save most recent tweets

                except Exception as e:
                    # Error... We will have to wait
                    #waiting = True
                    print str(e)
                    #print(traceback.format_exc())
                    print "No more queries remaining, sleeping for " + str(waitTime) +" seconds..."
                    time.sleep(waitTime)

            return new_tweets

        # Connecting to the API
        api = self._connectToAPI()

        #initialize a list to hold all the tweepy Tweets
        alltweets = []

        #make initial request for most recent tweets (200 is the maximum allowed count)
        """waiting = True
        while waiting == True:
            try:
                new_tweets = api.user_timeline(screen_name = screen_name,count=200)
                waiting = False
            except:
                # Error... We will have to wait
                waiting = True
                time.sleep(waitTime)  """
        new_tweets = _getNewTweets(api, screen_name)

        alltweets.extend(new_tweets)
        # Storing manually all the json representation for the tweets
        jTweets = []
        for n in new_tweets:
            jTweets.append(n._json)
        if len(alltweets) > 0:
            #save the id of the oldest tweet less one
            oldest = alltweets[-1].id - 1

            #keep grabbing tweets until there are no tweets left to grab
            while len(new_tweets) > 0:
                print "Getting tweets before %s" % (oldest)

                """ #all subsequent requests use the max_id param to prevent duplicates
                waiting = True
                while waiting == True:
                    try:
                        # We have to update the oldest id
                        new_tweets = api.user_timeline(screen_name = screen_name,count=200, max_id=oldest)
                        waiting = False
                        #save most recent tweets

                    except:
                        # Error... We will have to wait
                        waiting = True
                        print "No more queries remaining, sleeping for " + str(waitTime) +" seconds..."
                        time.sleep(waitTime)  """

                new_tweets = _getNewTweets(api, screen_name, oldest=oldest)

                # Extending the list of tweets
                alltweets.extend(new_tweets)

                #update the id of the oldest tweet less one
                oldest = alltweets[-1].id - 1
                print "... %s tweets downloaded so far" % (len(alltweets))
                # Storing manually all the json representation for the tweets
                for n in new_tweets:
                    jTweets.append(n._json)
        else:
            # Verifying the limits of the API
            print json.dumps(self._rate_limit_status(api=api, mode="get_all_docs"), indent =2)

        #transform the tweepy tweets into a 2D array that will populate the csv
        outtweets = []
        # This is how it is represented
        """
          "status": {
            "lang": "es",
            "favorited": false,
            "entities": {
              "symbols": [],
              "user_mentions": [],
              "hashtags": [],
              "urls": []
            },
            "contributors": null,
            "truncated": false,
            "text": "Podemos confirmar que Alpify, aunque acabe en ...fy no es una aplicaci\u00f3n nuestra. ;) \u00a1A aprovechar lo que queda de domingo!",
            "created_at": "Sun Aug 16 17:35:37 +0000 2015",
            "retweeted": true,
            "in_reply_to_status_id_str": null,
            "coordinates": null,
            "in_reply_to_user_id_str": null,
            "source": "<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>",
            "in_reply_to_status_id": null,
            "in_reply_to_screen_name": null,
            "id_str": "632968969662689280",
            "place": null,
            "retweet_count": 1,
            "geo": null,
            "id": 632968969662689280,
            "favorite_count": 0,
            "in_reply_to_user_id": null
          },
        """
        for tweet in jTweets:
            row =[]
            row.append(tweet["id_str"])
            row.append(tweet["created_at"])
            row.append(tweet["text"].encode("utf-8"))
            row.append(tweet["source"])
            row.append(tweet["coordinates"])
            row.append(tweet["retweet_count"])
            row.append(tweet["favorite_count"])
            row.append(tweet["lang"])
            row.append(tweet["place"])
            row.append(tweet["geo"])
            row.append(tweet["id"])
            row.append(screen_name)

            # URLS
            urls = []
            """
            [
                {
                  "url": "http://t.co/SGty7or6SQ",
                  "indices": [
                    30,
                    52
                  ],
                  "expanded_url": "http://github.com/i3visio/osrframework",
                  "display_url": "github.com/i3visio/osrfra\u2026"
                }
            ]
            """
            for u in tweet["entities"]["urls"]:
                urls.append(u["expanded_url"])
            # Creating the string value for the cell
            str_urls =""
            if len(urls) == 0:
                str_urls = "[N/A]"
            else:
                for i, u in enumerate(urls):
                    str_urls += u
                    # Appending a separator
                    if i+1 <> len(urls):
                        str_urls+= "|"
            row.append(str_urls.encode('utf-8'))

            # TODO: Extract Mentions
            mentions = []
            """ "user_mentions": [
              {
                "id": 66345537,
                "indices": [
                  0,
                  10
                ],
                "id_str": "66345537",
                "screen_name": "muchotomy",
                "name": "Tomy"
              },
            """
            for a in tweet["entities"]["user_mentions"]:
                mentions.append(a["screen_name"])
            # Creating the string value for the cell
            str_mentions =""
            if len(mentions) == 0:
                str_mentions = "[N/A]"
            else:
                for i, m in enumerate(mentions):
                    str_mentions += m
                    # Appending a separator
                    if i+1 <> len(mentions):
                        str_mentions+= "|"
            row.append(str_mentions.encode('utf-8'))

            # Appending the row to the output
            outtweets.append(row)

        # Writing the csv
        with open('%s_tweets.csv' % screen_name, 'wb') as f:
            writer = csv.writer(f)
            # Writing the headers
            writer.writerow([
                "_tweet_id",
                "_tweet_created_at",
                "_tweet_text",
                "_tweet_source",
                "_tweet_coordinates",
                "_tweet_retweet_count",
                "_tweet_favourite_count",
                "_tweet_lang",
                "i3visio_location",
                "_tweet_geo",
                "_twitter_id",
                "i3visio_alias",
                "i3visio_uri",
                "i3visio_alias_mentions",
            ])
            # Writing the rows
            #writer.writerows(outtweets)
            for o in outtweets:
                try:
                    writer.writerow(o)
                except:
                    print o

        return jTweets

    def get_followers(self, query):
        """
            Method to get the followers of a user.

            :param query:   Query to be performed.

            :return:    List of ids.
        """
        # Connecting to the API
        api = self._connectToAPI()

        # Verifying the limits of the API
        self._rate_limit_status(api=api, mode="get_followers")

        # Making the call to the API
        try:
            friends_ids = api.followers_ids(query)
        except:
            return []

        """res = []
        # Extracting the information from each profile
        for a in aux:
            us= self.getUser(a)
            res.append(self._processUser(us))"""

        return friends_ids

    def get_friends(self, query):
        """
            Method to get the friends of a user.

            :param query:   Query to be performed.

            :return:    List of users.
        """
        # Connecting to the API
        api = self._connectToAPI()

        # Verifying the limits of the API
        self._rate_limit_status(api=api, mode="get_friends")

        # Making the call to the API
        try:
            friends_ids = api.friends_ids(query)
        except:
            return []

        """res = []
        # Extracting the information from each profile
        for a in aux:
            us= self.getUser(a)
            res.append(self._processUser(us))"""

        return friends_ids


    def get_user(self, screen_name):
        """
            Method to perform the usufy searches.

            :param screen_name: nickname to be searched.

            :return:    User.
        """
        # Connecting to the API
        api = self._connectToAPI()

        # Verifying the limits of the API
        self._rate_limit_status(api=api, mode="get_user")

        aux = []
        try:
            user = api.get_user(screen_name)
            # Iterate through the results using user._json
            aux.append(user._json)
        except tweepy.error.TweepError as e:
            pass

        res = []
        # Extracting the information from each profile
        for a in aux:
            res.append(self._processUser(a))
        return res

    def search_users(self, query, n=20, maxUsers=60):
        """
            Method to perform the searchfy searches.

            :param query:   Query to be performed.
            :param n:   Number of results per query.
            :param maxUsers:    Max. number of users to be recovered.

            :return:    List of users
        """
        # Connecting to the API
        api = self._connectToAPI()

        # Verifying the limits of the API
        self._rate_limit_status(api=api, mode="search_users")

        aux = []

        page = 0

        # print "Getting page %s of new users..." % page+1
        # Making the call to the API
        try:
            newUsers = api.search_users(query, n, page)

            for n in newUsers:
                aux.append(n._json)

            #keep grabbing tweets until there are no tweets left to grab
            while len(aux) < maxUsers & len(newUsers)>0:
                page+=1
                print "Getting page %s of new users..." % page

                # Grabbing new Users
                newUsers = api.search_users(query, n, page)

                # Save the users found
                aux.extend(newUsers)
        except:
            pass

        res = []
        # Extracting the information from each profile
        for a in aux:
            res.append(self._processUser(a))

        return res


def main(args):
    """
        Query manager.
    """
    # Creating the instance
    tAW = TwitterAPIWrapper()

    # Selecting the query to be launched
    if args.type == "get_all_docs":
        results = tAW.get_all_docs(args.query)

    elif args.type == "get_user":
        results = tAW.get_user(args.query)

    elif args.type == "get_followers":
        results = tAW.get_followers(args.query)

        print "... %s followers downloaded... " % (len(results))
        #write the csv
        with open('%s_followers.csv' % args.query, 'wb') as f:
            writer = csv.writer(f)
            for r in results:
                writer.writerow([args.query,str(r)])

    elif args.type == "get_friends":
        results = tAW.get_friends(args.query)
        print "... %s friends downloaded... " % (len(results))
        #write the csv
        with open('%s_friends.csv' % args.query, 'wb') as f:
            writer = csv.writer(f)
            for r in results:
                writer.writerow([args.query,str(r)])

    elif args.type == "search_users":
        results = tAW.search_users(args.query)

    return results

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='A library that wraps searches onto the Twitter API.', prog='twitter_api.py', epilog="NOTE: the API tokens will be searched in api_keys.cfg configuration file.", add_help=False)
    # Adding the main options
    # Defining the mutually exclusive group for the main options
    parser.add_argument('-q', '--query', metavar='<hash>', action='store', help='query to be performed to the Twitter API.', required=True)
    parser.add_argument('-t', '--type', action='store', choices=["get_all_docs", "get_user", "get_followers", "get_friends", "search_users"], help='Type of query to be performed.', required=True)

    groupAbout = parser.add_argument_group('About arguments', 'Showing additional information about this program.')
    groupAbout.add_argument('-h', '--help', action='help', help='shows this help and exists.')
    groupAbout.add_argument('--version', action='version', version='%(prog)s 0.1.0', help='shows the version of the program and exists.')

    args = parser.parse_args()

    results = main(args)

    print json.dumps(results, indent=2)
